\chapter{\label{searchcw}Searching for continuous gravitational waves}
%%%%

%--------------------
% introduce problems in searching for CW
%---------------------
Continuous gravitational waves have particular challenges when it comes to their detection.
\glspl{CW} are long duration, this means that they would be observed for the entirety of a detectors observing runs. 
The signals also have an intrinsically small amplitude which is below the noise floor of current ground based detectors such as \gls{LIGO}.
This means the for a detection, the entire observing runs data will be needed to accumulate enough \gls{SNR} for a signal to be observed.
Given that \gls{LIGO} samples as $\sim 16$ kHz (generally downsampled to $\sim 4$ kHz) this leaves a huge amount of data which needs to be searched through.
As will be described in Sec.~\ref{searchcw:search}, this requires a large amount of computational resources to perform these searches.
For some types of search the parameter space can also be very large, this only adds to the computational time, which in some cases makes the search infeasible.

%---------------------
% Describe this chapter 
%--------------------------
Whilst I have described the potential sources of the signal and its approximate signal type in Sec.~\ref{intro:sources:cw},to perform a search the wave-form of a signal and how it is observed is needed.
In this section I will go into more detail on the `mountain' model in Sec.~\ref{intro:source:cw:mountain} and its wave-form description. 
This model is then used in various search methods for \gls{CW} signals.
In Sec.~\ref{searchcw:search} I will overview a subset of current searches for \gls{CW} signals.
Sec.~\ref{searchcw:motivation} explains the motivation for the majority of the work in this thesis.

%%%%%
%%%%%
\section{\label{searchcw:model}Continuous signal model}
%%%%
%%%%

The model of a \gls{GW} signal from a pulsar is relatively simple, it is a quasi-sinusoidal signal. This means that the signal is a sinusoid with a slowly varying frequency. One reason for the slow variance in the frequency is due to the energy loss to \gls{GW} as the pulsar spins down.
Here the signal is modelled to originate from an isolated triaxial neutron star rotating around a principal axis. 
The parameters of each pulsar can be split into two sections: the Doppler components ($\alpha,\delta,{\bm f}$) and its amplitude components ($\psi,\phi_0, \iota, h_0, \theta$). This ignores any orbital parameters which would be present if the star was in a binary systems and higher order frequency derivatives.
They are defined as follows: the sky positions $\alpha$ and $delta$ refer to the right ascension and declination. 
${\bm f}$ refers to the source frequency and its derivatives. 
$\psi$ and $\phi_0$ and $h_0 $ are the \gls{GW} polarisation, initial phase and amplitude respectively. 
$\iota$ is the inclination angle which is how much the source is tilted relative to the observer. 
$\theta$ is the `wobble angle' or the angle between the rotation axis and the symmetry axis of the neutron star.

The definition of the \gls{GW} from a neutron star here follows that in \citep{riles2017RecentSearches,schutz1998DataAnalysis,dupuis2005BayesianEstimation}. The amplitude of the \gls{GW} can be defined as
\begin{equation}
\label{intro:cw:ht}
h(t) = F_+(t)h_{+}(t) +F_{\times}(t)h_{\times}(t),
\end{equation}
where $h_{+},h_{\times}$ are the plus and cross polarisations functions as in Eq.\ref{intro:gw:gravwave} and $F_{+},F_{\times}$ are the antenna pattern functions to the two polarisations.
These are defined by
\begin{equation}
\label{intro:cw:amplitudes}
    \begin{split}
        h_{+}(t) &=  h_0 \frac{1 + \cos^2{(\iota)}}{2}\cos{\left(\Phi(t)\right)} \\
        h_{\times}(t) &= h_0  \cos{(\iota)} \sin{\left( \Phi(t)\right) } \\
    \end{split}
\end{equation}
The plus and cross polarised components then depend on the \gls{GW} amplitude $h_0$, the inclination angle of the source $\iota$ and the phase evolution of the \gls{GW}. Here I have chosen to assume a small wobble angle $\theta$, however, this is included in \citep{schutz1998DataAnalysis}. The phase of the wave $\Phi(t_{{\rm SSB}})$ at the \gls{SSB} can be defined as
\begin{equation}
\label{searchcw:model:phase}
    \Phi(t_{{\rm SSB}}) = \phi_0 + 2\pi\left[ f(t_{{\rm SSB}} - t_0) + \frac{1}{2} \dot{f} (t_{{\rm SSB}} - t_0)^2 + .....\right] .
\end{equation}
This consists of an initial phase $\phi_0$ which is the phase at time $t_0$, the frequency of the signal $f$ and its derivative ${\dot{f}}$ at time $t_0$. Here we show the phase to second order, however, this can be easily extended if necessary. 
The time at the \gls{SSB} $t_{{\rm SSB}}$ can be transformed to the time $t$ at the detector by
\begin{equation}
t_{{\rm SSB}} = t - \frac{\mathbf{r}_d \cdot \mathbf{k}}{c} + \delta_t.
\end{equation}
Here $\mathbf{r}_d$ is the position of the detector with reference to the \gls{SSB}, $\mathbf{k}$ is a unit vector in the direction of the source. This essentially takes into account the Doppler shift of the signal due to the movement of the detector, i.e. as the earth rotates and orbits the sun. $c$ is the speed of light and $\delta t$ is extra corrections from the Einstein, Binary and Shapiro delay \citep{}.
The amplitudes $h_0$ in Eq.~\ref{intro:cw:amplitudes} are defined by
\begin{equation}
    h_0 = \frac{16 \pi^2 G}{c^4} \frac{\epsilon I f^2}{r},
\end{equation}
where $G$ is the gravitational constant, $c$ is the speed of light, $\epsilon$ is the ellipticity of the star, $f$ is the sum of the frequency of rotation of the star and the frequency of precession, $r$ is the distance to the star and $I_{zz}$ is the moment of inertia with respect to the rotation axis $z$.
The ellipticity of the star $\epsilon$ is a measure of the distortion of the star around its rotation axis and is defined by
\begin{equation}
    \epsilon = \frac{I_{xx} - I_{yy}}{I_{zz}},
\end{equation}
where $I_{xx}, I_{yy}$ and $I_{zz}$ are the moments of inertia for each axis.

In Eq.~\ref{intro:cw:ht}, $F_+(t)$ and $F_{\times}(t)$ are the antenna pattern functions of the detector. 
These describe how sensitive a detector is to a particular location on the sky at any given time. 
The amplitude of the signal will vary dependent on the orientation and location of the detector relative to the source.
This is described in Sec.~\ref{intro:detector} and the response to sky location is shown in Fig.~\ref{intro:detectors:response}.
These components are defined in \citep{schutz1998DataAnalysis} as
\begin{equation}
\label{intro:cw:antenna}
\begin{split}
F_{+}(t) &= \sin{\zeta}[a(t)\cos{(2\psi)} + b(t)\sin{(2\psi)}], \\
F_{\times}(t) &= \sin{\zeta}[b(t) \cos{(2\psi)} - a(t)\sin{(2\psi)}],
\end{split}
\end{equation}
where $\zeta$ is the angle between the arms of the detectors, $\psi$ is the polarisation angle of the \gls{GW} and $a(t)$ and $b(t)$ are defined in \citep{schutz1998DataAnalysis} and relate the sky location to the orientation of the detector at a given time. 
A full derivation of this can be found in \citep{schutz1998DataAnalysis} where each of these terms are expanded.

Eq.~\ref{intro:cw:ht} - \ref{intro:cw:antenna} then describe the amplitude and phase evolution of a signal at a given detector location and orientation.



%%%%%%%%%%%%%
%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%
\section{\label{intro:prob} Bayes Theorem}
%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%

A key part in understanding the different methods to search for \gls{GW} or any data analysis, is understanding probability and statistics. 
This gives understanding of the random processes underlying all measured quantities. 
Whilst there are generally two approaches to statistics: Frequentist and Bayesian, here I will focus on the Bayesian approach.  

%%%%%%%%%%%%%
%%%%%%%%%%%%%
\subsection{\label{intro:prob:basic}Basic probability}
%%%%%%%%%%%%%%
%%%%%%%%%%%%%%

\joe{might remove this part}
Initially I will define some basic concepts of probability.
We can define the probability of some event $A$ as $p(A)$ where probabilities follow $0 \leq p(A) \leq 1$ and some other event $B$ which has a probability $p(B)$ and follows $0 \leq p(B) \leq 1$.

\begin{description}
	\item [Union]
	A union is the probability of either and event $A$ happening or event $B$ happening. This is written as, $p(A \cup B)$.
	
	\item [Intersection]
	An intersection is then the probability that both and event $A$ and an event $B$ happens. This is written as $p(A \cap B)$.
	
	\item [Independent and dependent Events]
	If the events $A$ and $B$ are independent, i.e. the event $A$ does not affect the outcome of event $B$, then
	\begin{equation}
	p(A \cap B) = p(A)p(B).
	\end{equation}
	However, if the event $A$ is dependent on event $B$, i.e. the event $A$ affects event $B$ or vice versa, then the joint probability of both events is
	\begin{equation}
	\label{dependentevent}
	p(A \cap B) = p(A)p(B \mid A) = p(B)p(A \mid B).
	\end{equation}
	Here $p(B \mid A)$ means the probability of event $B$ happening given that event $A$ has happened.
	
	\item [Conditional probability]
	Conditional probability arises from situations where the outcome of one event will affect the outcome of future events.
	The definition of this arises from the the dependent events defined above in Eq.~\ref{dependentevent}
	\begin{equation}
	p(A \mid B) = \frac{p(A \cap B)}{p(B)}.
	\end{equation}
	
	\item [Bayes Theorem]
	Bayes theorem can then be defined using conditional probabilities. i.e we can use
	\begin{equation}
	p(A \mid B) = \frac{p(A \cap B)}{p(B)} \quad \rm{and} \quad p(B \mid A) = \frac{p(A \cap B)}{p(A)}
	\end{equation}
	such that
	\begin{equation}
	p(B)p(A \mid B) = p(A)p(B \mid A)
	\end{equation}
	and this is rearranged to Bayes theorem
	\begin{equation}
	p(A \mid B) = \frac{p(A)p(B \mid A)}{p(B)}
	\end{equation}
	
\end{description}

%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%
\subsection{\label{intro:prob:bayes}Bayesian Inference}
%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%

We can take Bayes theorem from Sec.~\ref{intro:prob:basic} and apply it to a problem which involves inferring some parameters from some model. Here we can relabel the events $A$ and $B$ with the data ${\bm d}$ and the parameters ${\bm \theta}$ of some model $I$.
Equation \ref{intro:prob:bayes} then becomes
\begin{equation}
\label{intro:bayes:bayes}
p({\bm \theta} \mid {\bm d}, I) = \frac{p({\bm \theta}, I)p({\bm d} \mid {\bm \theta}, I)}{p({\bm d} \mid I)}
\end{equation}
where each of the components are assigned names: $p({\bm \theta} \mid {\bm d})$ is the posterior distribution, $p({\bm \theta})$ is the prior distribution,  $p({\bm d} \mid {\bm \theta})$ is the likelihood and $p({\bm d})$ is the evidence.

\begin{description}
	\item [Posterior]
	The posterior distribution describes the probability of a parameter ${\bf \theta}$ in some model $I$ given some data $d$. For many problems this is the distribution which is most useful as it informs you the most likely set of parameters of you model given some observation.
	\item [Prior]
	The Prior distribution is a key part of Bayesian statistics. This distribution describes any information which you have prior to the observation. This is a distribution defined by the user, where you define a distribution of the parameters based on what you expect to be true.
	\item [Likelihood]
	The likelihood is where the observation is included in the calculation. This tells you how probable it is to get the observed data $d$ given the model $I$ with the set of parameters $\theta$. 
	\item [Evidence]
	The evidence is the probability of the data itself given the choice of model. This is found by integrating the likelihood over all possible values of ${\bm \theta}$ weighting them by our prior belief of that value of ${\bm \theta}$. This known as a marginal distribution and is defined by,
	\begin{equation}
	\label{intro:bayes:evidence}
	p({\bm d} \mid I) = \int p({\bm \theta}, I)p({\bm d} \mid {\bm \theta}, I) d{\bm \theta}.
	\end{equation}
\end{description}


Bayes theorem then gives a description of the probability distribution of some parameters in a model given some observation.
Often when using Bayesian statistics the aim is to find posterior distribution of parameters.
There are very few cases where this can be calculated analytically, therefore, numerical methods are almost always used to find the posterior.
This can be difficult to calculate numerically especially in problems where the parameters space has many dimensions.
The most difficult part to calculate is the evidence in Eq.~\ref{intro:bayes:evidence}, this involves calculating an integral over all possible parameters.
There is however, a way around having to calculate this. For any given model $I$, the evidence $p({\bm d}\mid I)$ is the same for any set of parameters ${\bm \theta}$ in Eq.~\ref{intro:bayes:bayes}. 
The evidence is then just a normalisation factor for the posterior distribution. 
When different models are not being compared, and we assume the model $I$ to be true, we no longer need to calculate the evidence.
The unnormalised posterior distribution can then be found by sampling
\begin{equation}
p({\bm \theta} \mid {\bm d}, I) \propto p({\bm \theta}, I)p({\bm d} \mid {\bm \theta}, I).
\end{equation}
To numerically approximate this posterior you could then calculate (sample) the value over a grid of points parameter space, which can quickly become computationally expensive. Often the posterior distribution is located in a small area within the parameter space, so the majority of the computational time is spent sampling a area of parameter space where the posterior is close to zero.
Most problems are interested in areas of high probability, i.e the larger values of the posterior. 
A method titled \gls{MCMC} can be used to concentrate the samples around areas of high probability, which can approximate the posterior distribution more efficiently, more information on this can be found in \citep{metropolis1953EquationState,vanravenzwaaij2018SimpleIntroduction,sharma2017MarkovChain}.
This builds up the posterior distribution by using a Markov chain, where each step in the chain only depends on the previous step.
It starts by calculating the posterior value for a particular point in parameter space. Then it will randomly jump to another parameter space point, where a new value for the posterior can be calculated.
If the new posterior value is higher than the previous step then the jump is `accepted'. This just means that the parameter values of this point are stored.
If the posterior value is lower than the previous step then the jump is accepted with some probability.
This means that the accepted positions are located around areas of high posterior values, the \gls{MCMC} algorithm does not waste time calculating the posterior in uninteresting areas of parameter space.
The accepted samples then build the posterior distribution. 

In certain situations it can be useful to calculate the evidence in Eq.\ref{intro:bayes:evidence}. 
For example, if there are two different models which could represent the data, the evidence can be used to determine which of the two models is more likely.
This is known as a Bayes factor where two models $I_1$ and $I_2$ are compares and is defined as
\begin{equation}
B = \frac{p({\bm d} \mid I_1)}{p({\bm d} \mid I_2)}
\end{equation}
This then requires the calculation of the evidence.
To estimate the evidence efficiently a method known as Nested sampling can be used, this is explained in detail in \citep{skilling2006NestedSampling,speagle2019DynestyDynamic}.
By calculating the Bayes factor, which is similar to a likelihood ratio, one can find the posterior odds of a particular model by using,
\begin{equation}
\frac{p(I_1 \mid \mathbf{d})}{p(I_2 \mid \mathbf{d})} = \frac{p(I_1)}{p(I_2)} \frac{p(\mathbf{d} \mid I_1)}{p(\mathbf{d} \mid I_2)}.
\end{equation}
The can be written as \textit{posterior odds = prior odds $\times$ Bayes factor}. This is then a comparison of how likely different models are given some observation. 

The methods described above then provide a way to estimate parameters of a model given some data. 
Also this provides a way to compare different models given some observation.
In following sections the methods described above are used to estimate various parameters.


%%%%%%%%%%%55
%%%%%%%%%%%%%
\section{\label{searchcw:search} Continuous wave searches}
%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%

Searches for \glspl{CW} can be split into three general categories: Targeted searches, Directed searches and All-sky searches, where the different categories are based on the amount of source parameters which are known prior to running the search.

%%%%%%
%%%%%%
\subsection{\label{searchcw:search:targeted}Targeted}
%%%%%%
%%%%%%

Targeted searches search for specific pulsars which have parameters known from electromagnetic observations, i.e. X-ray, radio or $\gamma$-ray.
These can give the sky position parameters $\alpha$ and $\delta$ and the source frequency parameters $f$ and its derivatives, where there could also be extra parameters if the pulsar is in a binary system.
Targeted searches can then use these parameters as input such that they search over the remaining parameters $h_0, \iota, \phi_0, \psi$.
The main targeted searches are the Bayesian time-domain search \citep{dupuis2005BayesianEstimation}, the matched filter $\mathcal{F}$-statistic \citep{schutz1998DataAnalysis} and the 5-Vector approach \citep{astone2010MethodDetection}.

The Bayesian time-domain search is based on reducing the dataset to a manageable size such that Bayesian parameter estimation can be applied with a reasonable computational cost. 
This search uses sky position and frequency parameters of the source to perform a slowly evolving heterodyne which removes the phase evolution of the source \citep{dupuis2005BayesianEstimation}.
This allows the signal to be low pass filtered and heavily downsampled without losing any of the signals information.
This reduced dataset can then be used in a Bayesian approach to search over the parameters $h_0, \iota, \phi_0, \psi$, see \citep{dupuis2005BayesianEstimation} for further information.
The matched filter $\mathcal{F}$-statistic uses the same heterodyned data as the Bayesian method, howeverm, calculates a maximum likelihood $\mathcal{F}$-statistic instead of a Bayesian posterior \citep{schutz1998DataAnalysis,prix2007SearchContinuous, aasi2014GRAVITATIONALWAVES}. It uses the same heterodyned data as the Bayesian method, but computes a maximum likelihood statisic

The 5-Vector search is based in the frequency domain, which makes use of the five frequency harmonics caused by the sidereal amplitude modulation caused by the detectors antenna response as the earth rotates \citep{astone2010MethodDetection,aasi2014GRAVITATIONALWAVES}. 
A summary of the application of the searches for initial and advanced \gls{LIGO} can be found in \citep{aasi2014GRAVITATIONALWAVES,abbott2019SearchesGravitationala}.

Due to the long observation times needed to accumulate the required \gls{SNR} for detection, most searches use data from an entire \gls{LIGO} observing run which can last for $\mathcal{O}(1)$ years.
Given that the sampling rate for the \gls{GW} channel is $16$ kHz (often downsampled to $\sim 4$ kHz), the amount of data in a year can be upt to $\mathcal{O}(2)$ terabytes, therefore these types of search can be computationally costly. 
Whilst the fully coherent matched filter searches have methods to reduce the computational time for known sources, in all-sky and directed searches, this type of search is no feasible. 
This is because all-sky and directed searches have a wider parameter space, therefore, enough templates need to be made to sufficiently cover the large parameter space. 
This task quickly becomes impossible for coherent matched filtering for an entire observing run due to the amount of time needed. This problem led to the development of semi-coherent searches which will be introduced in the next section. 

%%%%%%
%%%%%%
\subsection{\label{searchcw:search:directed}Directed}
%%%%%%
%%%%%%
Directed searches generally know some of the source parameters such as the sky position $(\alpha,\delta)$ but not the rotation frequency. This includes searches for neutron stars in binary systems such as Sco-X1 \citep{abbott2017UpperLimits,meadors2016TuningScorpius}.
These searches use the similar techniques as all-sky searches, which will be described in Sec.~\ref{searchcw:search:allsky}, they differ in that they can limit the parameter space based on the known parameters.

%%%%%%
%%%%%%
\subsection{\label{searchcw:search:allsky}All-sky searches}
%%%%%%
%%%%%%

All-sky searches have no prior knowledge of the pulsars parameters, therefore, search over all the pulsar parameters $h_0, \iota, \psi, \phi_0, f, \dot{f}, \alpha, \delta$.
To use the techniques described in Sec.~\ref{searchcw:search:targeted} for an all sky search, to sufficiently cover the entire parameter space, the search would not be feasible to run due to its large computational cost.
Instead a type of search known as a semi-coherent search was developed.
These offered a solution to searching over the large parameters space and data size. 
The general idea of a semi-coherent search is to break the dataset into smaller segments of length $T_{coh}$, which each can be analysed coherently.
The techniques described in Sec.~\ref{searchcw:search:targeted} or another method such as a Fourier transform can be used to analyse these small segments.
The results from each segment can the be combined incoherently using various methods which will be summarised below.
This method can greatly reduce the computational cost for the analysis depending on the coherence length, but will reduce the sensitivity compared to the targeted methods. 

There are many different types of semi-coherent search which use various methods to incoherently combine the coherently analysed results. 
I will summarise some of these searches below, some of these searches were summarised and compared in \citep{walsh2016ComparisonMethods}.
Many of these searches use a set of 1800s long Fourier transforms as the input data, known as \glspl{SFT}. This is a default for many all-sky \gls{CW} searches, where it assumes that the signal remains within one frequency bin during that 1800s.

\begin{description}
	
	\item[Stack-slide] Stack uses a set of Fourier transforms of the data known as \glspl{SFT}, specifically it uses the power spectrum of these. Each of the separate Fourier transforms (segments) is shifted up or down relative to the others to account for the Doppler modulation of the source. The power from each can then be stacked. More explanation of this can be found in \citep{brady2000SearchingPeriodic, cutler2005ImprovedStackslide}  
	
	\item[Hough] The Hough transform is based on the stack-slide algorithm. The main difference is that the detection statistic for each segment is assigned a weight of 0 or 1 depending if it crossed a detection threshold. The Hough transform can the create a `Hough map' which gives a view of the data in parameter space. This approach is explained in greater detail in \citep{krishnan2004HoughTransform,antonucci2008DetectionPeriodic}. 
	This method has been applied in two main ways known as Sky Hough \citep{krishnan2004HoughTransform}and Frequency Hough \citep{antonucci2008DetectionPeriodic,astone2014MethodAllsky}.
	
	\item[Einstein@Home] Einstein at home uses the $\mathcal{F}$-statistic mentioned above in various stages. It has a hierarchical structure where it starts with a coarse parameter space with shorter coherence times. This search then provides a list of candidates from this run in coarse parameter space. The parameter space is then more finely sampled around the parameters of the candidates and this process is repeated. The search can also increases the coherence length when searching around given candidates to improve the sensitivity of the search. This algorithm has many additions which are explained in more detail in \citep{singh2016ResultsAllsky,papa2016HierarchicalFollowup,walsh2016ComparisonMethods}
	This provides the most sensitive all-sky \gls{CW} search, however, uses a large amount of computing power. This is achieved by using a distributed computing projects, more details can be found at \citep{EinsteinHome}. 
	
	\item[Time domain $\mathcal{F}$-statistic] The time domain $\mathcal{F}$-statistic splits the data into narrowband segments of length $\sim$ 2 days \citep{walsh2016ComparisonMethods}. Then a coherent search using the $\mathcal{F}$-statistic is applied to each of these segments. Values of this statistic above a threshold are stored. Coincidences are then found in each segment, where candidates are selected best on a given threshold. This is explained in greater detail in \citep{aasi2014ImplementationTextdollar,walsh2016ComparisonMethods}.
	
	
	\item[Powerflux] Powerflux uses a standard set of 1800s \glspl{SFT}. For each point in parameter space, the power in this set of \glspl{SFT} along the frequency track is recorded. This power is then weighted depending on the antenna pattern and noise of the detector. In longer stretches of $\sim$ 1 month, the weighted power is summed. Any point in parameter space which produces high power in each of these stretches is identified as a potential signal. This search can then be repeated around each candidate with a finer resolution in parameter space. This is explained in more detail and tested in \citep{abadie2012AllskySearch,walsh2016ComparisonMethods,ligoscientificcollaborationandvirgocollaboration2016ComprehensiveAllsky}
	
	\item[Viterbi] The Viterbi algorithm \citep{viterbi1967ErrorBounds} has been used in \citep{sun2018HiddenMarkov, suvorova2017HiddenMarkov,abbott2017SearchGravitational, abbott2018SearchGravitational, sun2018ApplicationHidden} to search for a \glspl{CW} with randomly wandering spin frequency. This algorithm was applied to specific sources, where the $\mathcal{F}$-statistic is used on short duration segments which are then incoherently combined using the Viterbi algorithm.
	
	
\end{description}


Each of these searches uses a large computational cost. In \citep{walsh2016ComparisonMethods} a \gls{MDC} was conducted to compare the sensitivity of some of the searches, where an expected runtime for and O1 search was presented. Result in O1 for some of these searches can now be found in \citep{ligoscientificcollaborationandvirgocollaboration2017AllskySearch}.
The results of this are shown in Tab.\ref{searchcw:search:semi:cost}.
\begin{table}
	\centering
	%
	\caption[Computational cost of \gls{CW} searches.]{From \citep{walsh2016ComparisonMethods}, shows the computational cost for the first 4 months of advanced \gls{LIGO} for each search. One \gls{MSU}, where one standard unit is one core-hour on a standard core. ` Expected computational costs of searches using the first four months of advanced \gls{LIGO} data with each search pipeline. These estimates are for a different data observing time from that of the \gls{MDC}, and do not cover the same parameter space as each other or the \gls{MDC}. The Einstein@Home searches uses the computing resources of the Einstein@Home project and is designed to run for 6 - 10 months in the Einstein@Home grid.'  \label{searchcw:search:semi:cost}}
	
	%
	\bgroup
	\def\arraystretch{1.5}
	\centering
	\begin{tabular}{|c c|}
		\hline
		Pipeline & Expected runtime of O1 search \\
		\hline
		Powerflux & 6.8 MSU \\

		Time domain $\mathcal{F}$-statistic & 1.6 MSU\\

		Frequency Hough & 0.9 MSU \\

		Sky Hough & 0.9 MSU\\
		\hline
		Einstein@Home & 100-170 MSU\\
		\hline

	\end{tabular}
	\egroup
\end{table}
Even the fastest of these searches takes close to 1 million core-hours to search through four months of data.
This is one of the larger current issued in \gls{CW} searches as this is a slow process and running computing clusters can be costly.


%%%%%%%%%5
%%%%%%%%%%%
\section{\label{searchcw:motivation}Motivation}
%%%%%%%%%%
%%%%%%%%%%

The searches described in Sec.~\ref{searchcw:search} are computationally expensive, where the fastest takes $\sim$1 million core-hours to search through 4 months of O1 data.
Many of these searches use well-modelled signals to compare to the data. This leads to the parameter space having to be finely sampled such that it is sufficiently covered.  
The motivation for much of the work then follows from these points. 
The aim was to develop searches which used minimal computational resources and could work outside of the \gls{CW} model in Sec.~\ref{searchcw:model}.
This thesis then outlines algorithms which can reduce this computational time of searches for \gls{CW} whilst retaining as much sensitivity to \gls{CW} signals as possible.
There are two main sections which follow, Sec.~\ref{soap} will outline an essentially un-modelled \gls{CW} search method which uses the Viterbi algorithm.
Sec~\ref{machine} will then outline an method which used machine learning, specifically \glspl{CNN} as both its own \gls{CW} search and an extension to the search described in Sec.~\ref{soap}.
Following chapters then explain applications of these searches.


