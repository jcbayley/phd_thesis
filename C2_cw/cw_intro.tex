\chapter{\label{searchcw}Searching for continuous gravitational waves}
%%%%

%--------------------
% introduce problems in searching for CW
%---------------------
Continuous gravitational waves~\chris{are you using acronymns or not?} have
particular challenges when it comes to their detection.  \glspl{CW} are long
duration, this means that they would be observed for the entirety of a
detectors observing run.  The signals also have an intrinsically small
amplitude which is below the noise floor of current ground based detectors such
as \gls{LIGO}.  This means that for a detection, the entire observing
runs~\chris{apostophe for belonging to the run} data will be needed to
accumulate enough \gls{SNR} for a signal to be observed.  Given that \gls{LIGO}
samples at $\sim 16$ kHz (generally downsampled to $\sim 4$ kHz) this leaves a
huge amount of data which needs to be searched through~\chris{can you give a general
number? in GBs?}.  As will be described in Sec.~\ref{searchcw:search}, this
requires a large amount of computational resources to perform these searches.
For some types of search the parameter space can also be very large, this only
adds to the computational time, which in some cases makes the search
infeasible.

%---------------------
% Describe this chapter 
%--------------------------
Whilst I have described the potential sources of the signal and its approximate
signal type in Sec.~\ref{intro:sources:cw}, to perform a search the
wave-form~\chris{wave-form or waveform - be consistent throughout} of a signal
and how it is observed is needed~\chris{clunky sentence}.  In this
section~\chris{which section? this chapter - be specific} I will go into more
detail on the `mountain' model in Sec.~\ref{intro:source:cw:mountain} and its
wave-form description.  This model is then used in various search methods for
\gls{CW} signals~\chris{but the signal model from the mountain model is the
same signal model for all long duration CW signals - it just predicts a
sinusoid but some models predict the frequency at different values. The point s
that you are not only describing the waveform for one model}. In
Sec.~\ref{searchcw:search} I will overview a subset of current searches for
\gls{CW} signals.~\chris{and instead of fullstop?}
Sec.~\ref{searchcw:motivation} explains the motivation~\chris{OK but it would
be good if the thesis is motivated before page 29 and if that motivation spans
more than 1 paragraph} for the majority of the work in this thesis.

%%%%%
%%%%%
\section{\label{searchcw:model}Continuous signal model}
%%%%
%%%%

The model of a \gls{GW} signal from a pulsar~\chris{try to be careful with
pulsar vs NS statements. Which one do you really mean? A pulsar is a NS but a
NS is not required to be a pulsar} is relatively simple, it is a
quasi-sinusoidal signal~\chris{do you mean at the source (intrinsic properties)
or when detected?}. This means that the signal is a sinusoid with a slowly
varying frequency. One reason for the slow variance~\chris{variance means
something else so best use variation} in the frequency is due to the energy
loss to \gls{GW} as the pulsar spins down~\chris{Yes, that is possible BUT such
a spin-down would have a certain characteristic rate for GW emission - see
something like https://arxiv.org/abs/1607.05315 for example. It is
characterised by the "braking-index" which should be 5 for GWs and 3 for
magnetic braking. The expression is defined as $n = f\ddot{f}/\dot{f}^2$ and
for most pulsars it is measured to be below 3.}. Here the signal is modelled to
originate from an isolated triaxial neutron star rotating around a principal
axis.~\chris{new paragraph?} The parameters of each pulsar~\chris{again with
the pulsar vs NS} can be split into two sections: the Doppler components
($\alpha,\delta,{\bm f}$) and its amplitude components ($\psi,\phi_0, \iota,
h_0, \theta$)~\chris{redefine thgese quantities even if already defined
elsewhere}. This ignores any orbital parameters which would be present if the
star was in a binary systems and higher order frequency derivatives~\chris{be
clearer - what are higher order frequency derivatives?  You're talking about
intrinsic spin derivatives}. They are defined as follows: the sky positions
$\alpha$ and $\delta$ refer to the right ascension and declination~\chris{of
the source?}. \chris{Don't start sentences with variables} ${\bm f}$ refers to
the source frequency and its derivatives.  \chris{same here} $\psi$ and
$\phi_0$ and $h_0 $ are the \gls{GW} polarisation, initial phase and amplitude
respectively.  $\iota$ is the inclination angle which is how much the source is
tilted relative to the observer~\chris{that is too vague! Be more specific}.
$\theta$ is the `wobble angle' or the angle between the rotation axis and the
symmetry axis of the neutron star.

The definition of the \gls{GW} from a neutron star~\chris{given} here follows that in
\citep{riles2017RecentSearches,schutz1998DataAnalysis,dupuis2005BayesianEstimation}.
The amplitude of the \gls{GW} can be defined as
%
\begin{equation}
\label{intro:cw:ht}
h(t) = F_+(t)h_{+}(t) +F_{\times}(t)h_{\times}(t),
\end{equation}
%
where $h_{+},h_{\times}$ are the plus and cross polarisations functions as in
Eq.\ref{intro:gw:gravwave}~\chris{grammar} and $F_{+},F_{\times}$ are the antenna pattern
functions of the two polarisations. These are defined by
%
\begin{equation}
\label{intro:cw:amplitudes}
    \begin{split}
        h_{+}(t) &=  h_0 \frac{1 + \cos^2{(\iota)}}{2}\cos{\left(\Phi(t)\right)} \\
        h_{\times}(t) &= h_0  \cos{(\iota)} \sin{\left( \Phi(t)\right) }. \\
    \end{split}
\end{equation}
%
The plus and cross polarised components then depend on the \gls{GW} amplitude
$h_0$, the inclination angle of the source $\iota$~\chris{what is an
inclination angle? Extra definition required} and the phase evolution of
the \gls{GW}. Here I have chosen to assume a small wobble angle $\theta$,
however, this is included in \citep{schutz1998DataAnalysis}~\chris{grammar}.
The phase of the wave $\Phi(t_{{\rm SSB}})$ at the \gls{SSB} can be defined as
%
\begin{equation}
\label{searchcw:model:phase}
    \Phi(t_{{\rm SSB}}) = \phi_0 + 2\pi\left[ f_0(t_{{\rm SSB}} - t_0) + \frac{1}{2} \dot{f_0} (t_{{\rm SSB}} - t_0)^2 + .....\right] .
\end{equation}
%
This consists of an initial phase $\phi_0$ which is the phase at time $t_0$,
the frequency of the signal $f_0$ and its derivative ${\dot{f}}$ measured at
time $t_0$. Here we show the phase to second order, however, this can be easily
extended if necessary~\chris{strange way of saying this. Makes you think of
ansking what constitutes necessary}. The time at the \gls{SSB} $t_{{\rm SSB}}$ can be
transformed to the time $t$ at the detector by
%
\begin{equation}
	\label{searchcw:model:ssbtime}
t_{{\rm SSB}} = t + \frac{\mathbf{r}_d \cdot \mathbf{n}}{c} + \delta_t.
\end{equation}
%
Here $\mathbf{r}_d$ is the position of the detector with reference to the
\gls{SSB}, $\mathbf{n}$ is a unit vector in the direction of the source. This
essentially~\chris{essentially?} takes into account the Doppler shift of the
signal due to the movement of the detector, i.e. as the earth rotates and
orbits the sun. $c$ is the speed of light and $\delta t$ is~\chris{grammar?}
extra corrections from the Einstein, Binary, and Shapiro delay
\citep{}~\chris{add references - but also you should know what those are and be
prepared to explain them and/or include them in the thesis}. The amplitude
$h_0$ in Eq.~\ref{intro:cw:amplitudes} is defined by
%
\begin{equation}
    h_0 = \frac{16 \pi^2 G}{c^4} \frac{\epsilon I_{zz} f^2}{r},
\end{equation}
%
where $G$ is the gravitational constant, $c$ is the speed of light~\chris{you
just defined $c$ a second ago}, $\epsilon$ is the ellipticity of the star, $f$
is the sum of the frequency of rotation~\chris{be very careful with rotation
afrequency and GW frequency. The $f_0$ that you used in the phase definition
was GW frequency but now you're talking about rotation.} of the star and the
frequency of precession, $r$ is the distance to the star and $I_{zz}$ is the
moment of inertia with respect to the rotation axis $z$.  The ellipticity of
the star $\epsilon$ is a measure of the distortion of the star~\chris{multiple
"of the star"} around its rotation axis and is defined by
%
\begin{equation}
    \epsilon = \frac{I_{xx} - I_{yy}}{I_{zz}},
\end{equation}
%
~\chris{you have already defined this expression in Chapter 1. It's not crazy
to redefine it but just be aware. The same comments apply here as I mentioned
in Chapter 1 regarding the definitions of $I$} where $I_{xx}, I_{yy}$ and
$I_{zz}$ are the moments of inertia for each axis.

In Eq.~\ref{intro:cw:ht}, $F_+(t)$ and $F_{\times}(t)$ are the antenna pattern
functions of the detector.  These describe how sensitive a detector is to a
particular location on the sky at any given time.  The amplitude of the signal
will vary dependent on the orientation and location of the detector relative to
the source.  This is described in Sec.~\ref{intro:detector} and the response to
sky location is shown in Fig.~\ref{intro:detectors:response}.  These components
are defined in \citep{schutz1998DataAnalysis} as
%
\begin{equation}
\label{intro:cw:antenna}
\begin{split}
F_{+}(t) &= \sin{\zeta}[a(t)\cos{(2\psi)} + b(t)\sin{(2\psi)}], \\
F_{\times}(t) &= \sin{\zeta}[b(t) \cos{(2\psi)} - a(t)\sin{(2\psi)}],
\end{split}
\end{equation}
%
where $\zeta$ is the angle between the arms of the detectors, $\psi$ is the
polarisation angle of the \gls{GW} and $a(t)$ and $b(t)$ are defined in
\citep{schutz1998DataAnalysis} and relate the sky location to the orientation
of the detector at a given time.  A full derivation of this can be found in
\citep{schutz1998DataAnalysis} where each of these terms are expanded.

Eq.~\ref{intro:cw:ht} - \ref{intro:cw:antenna}~\chris{use Equation rather than
Eq. when startinga sentence. Also, this is a particualrly short and possibly
redundant paragraph} then describe the amplitude and phase evolution of a
signal at a given detector location and orientation.



%%%%%%%%%%%%%
%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%
\section{\label{intro:prob} Bayes Theorem}
%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%

A key part in understanding the different methods~\chris{used?} to search for
\gls{GW} or any~\chris{careful with words like "any". Are you absolutely sure
of your statement for "all" cases?} data analysis, is understanding probability
and statistics.  This gives~\chris{us?} understanding of the random processes
underlying all measured quantities.  Whilst there are generally two approaches
to statistics: Frequentist and Bayesian, here I will focus on the Bayesian
approach.  

%%%%%%%%%%%%%
%%%%%%%%%%%%%
\subsection{\label{intro:prob:basic}Basic probability}
%%%%%%%%%%%%%%
%%%%%%%%%%%%%%

\joe{might remove this part}~\chris{I really like it, keep it in}
Initially I will define some basic concepts of probability.  We can define the
probability of some event $A$ as $p(A)$ where probabilities follow $0 \leq p(A)
\leq 1$ and some other event $B$ which has a probability $p(B)$ and
follows~\chris{is follows the right word here?} $0 \leq p(B) \leq 1$.

\begin{description}
	\item [Union]
	A union is the probability of either event $A$ happening or event $B$ happening. This is written as, $p(A \cup B)$.
	
	\item [Intersection]
	An intersection is then the probability that both and event $A$ and an event $B$ happens. This is written as $p(A \cap B)$.
	
	\item [Independent and dependent Events]
	If the events $A$ and $B$ are independent, i.e. the event $A$ does not affect the outcome of event $B$, then
	\begin{equation}
	p(A \cap B) = p(A)p(B).
	\end{equation}
	However, if the event $A$ is dependent on event $B$, i.e. the event $A$ affects event $B$ or vice versa, then the joint probability of both events is
	\begin{equation}
	\label{dependentevent}
	p(A \cap B) = p(A)p(B \mid A) = p(B)p(A \mid B).
	\end{equation}
	Here $p(B \mid A)$ means the probability of event $B$ happening given
that event $A$ has happened~\chris{by using words with the past tense you are
implying that things have to be done in a certain order. Best to avoid that}.
	
	\item [Conditional probability]
	Conditional probability arises from situations where the outcome of one
event will affect the outcome of future~\chris{same here in talking about the
future} events.
	The definition of this arises from the the dependent events defined above in Eq.~\ref{dependentevent}
	\begin{equation}
	p(A \mid B) = \frac{p(A \cap B)}{p(B)}.
	\end{equation}
	
	\item [Bayes Theorem]
	Bayes theorem can then be defined using conditional probabilities. i.e we can use
	\begin{equation}
	p(A \mid B) = \frac{p(A \cap B)}{p(B)} \quad \rm{and} \quad p(B \mid A) = \frac{p(A \cap B)}{p(A)}
	\end{equation}
	such that
	\begin{equation}
	p(B)p(A \mid B) = p(A)p(B \mid A)
	\end{equation}
	and this is rearranged to Bayes theorem
	\begin{equation}
	p(A \mid B) = \frac{p(A)p(B \mid A)}{p(B)}
	\end{equation}
	
\end{description}

%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%
\subsection{\label{intro:prob:bayes}Bayesian Inference}
%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%

We can take Bayes theorem from Sec.~\ref{intro:prob:basic} and apply it to a
problem which involves inferring some parameters from some model. Here we can
relabel the events $A$ and $B$ with the data ${\bm d}$ and the parameters ${\bm
\theta}$ of some model $I$.  Equation \ref{intro:prob:bayes} then becomes
%
\begin{equation}
\label{intro:bayes:bayes}
p({\bm \theta} \mid {\bm d}, I) = \frac{p({\bm \theta}, I)p({\bm d} \mid {\bm \theta}, I)}{p({\bm d} \mid I)}
\end{equation}
%
where each of the components are assigned names: $p({\bm \theta} \mid {\bm d})$
is the posterior distribution, $p({\bm \theta})$ is the prior distribution,
$p({\bm d} \mid {\bm \theta})$ is the likelihood and $p({\bm d})$ is the
evidence.

\begin{description}
	\item [Posterior]
        The posterior distribution describes the probability of a parameter
${\bf \theta}$ in some model $I$ given some data $d$. For many problems this is
the distribution which is most useful as it informs you of the most
likely~\chris{careful - the most likely parameters are actualy those that
correspond to the peak of the posterior.} set of
parameters of you model given some observation.
	
        \item [Prior] The Prior distribution is a key part of Bayesian
statistics. This distribution describes any information which you have prior to
the observation. This is a distribution defined by the user, where you define a
distribution of the parameters based on what you expect to be true.~\chris{be a
bit more specific here - it's the prior distribution o the parameters given the
model and whilst you can choose them, it is sensible to have them really
reflect your beliefs prior to the experiment.}
	
        \item [Likelihood] The likelihood is where the observation is included
in the calculation. This tells you how probable it is to get the observed data
$d$ given the model $I$ with the set of parameters $\theta$. 
	
        \item [Evidence] The evidence is the probability of the data itself
given the choice of model. This is found by integrating the likelihood over all
possible values of ${\bm \theta}$ weighting them by our prior belief of that
value of ${\bm \theta}$. This~\chris{is also?} known as a marginal
distribution~\chris{no, the marginal likelihood} and is defined
by,
	%
        \begin{equation} \label{intro:bayes:evidence} 
            p({\bm d} \mid I) = \int p({\bm \theta}, I)p({\bm d} \mid {\bm \theta}, I) d{\bm \theta}.
        \end{equation} 
\end{description}

Bayes theorem then gives a description of the probability distribution of some
parameters in a model given some observation.  Often when using Bayesian
statistics the aim is to find posterior distribution of parameters.  There are
very few cases where this can be calculated analytically, therefore, numerical
methods are almost always~\chris{maybe tone down the "very few" and "almost
always" statements. Can you back these up?} used to find the posterior.  This
can be difficult to calculate numerically especially in problems where the
parameters space has many dimensions.  The most difficult part to calculate is
the evidence in Eq.~\ref{intro:bayes:evidence}, this involves calculating an
integral over all possible parameters.  There is however, a way around having
to calculate this.  For any given model $I$, the evidence $p({\bm d}\mid I)$ is
the same for any set of parameters ${\bm \theta}$ in
Eq.~\ref{intro:bayes:bayes}~\chris{that's a bit ambiguous - I would say that it
is independent of the parameters and is only dependent on the assumed model}.
The evidence is then just a normalisation factor for the posterior
distribution.  When different models are not being compared, and we assume the
model $I$ to be true, we no longer need to calculate the evidence.  The
unnormalised posterior distribution can then be found by sampling~\chris{maybe
you discuss this next but "sampling" won't make sense to the reader at this
stage.}
%
\begin{equation}
p({\bm \theta} \mid {\bm d}, I) \propto p({\bm \theta}, I)p({\bm d} \mid {\bm \theta}, I).
\end{equation}

To numerically approximate this posterior you could then calculate (sample) the
value over a grid of points parameter space, which can quickly become
computationally expensive. Often the posterior distribution is located in a
small area within the parameter space, so the majority of the computational
time is spent sampling a area of parameter space where the posterior is close
to zero.  Most problems are interested in areas of high probability, i.e the
larger values of the posterior~\chris{OK, correct but you should probably
mention the dimensionality cost here - also I can't think of any cases where
you'd be interested in areas of low probability. Theoretically we are
interested in mapping the entire parameter space and finding the values of the
probability everywhere. In practice we want to know where the probability is
high since that will correlate with the truth.}.  A method titled \gls{MCMC}
can be used to concentrate the samples around areas of high probability, which
can~\chris{be used to?} approximate the posterior distribution more
efficiently, more information on this can be found in
\citep{metropolis1953EquationState,vanravenzwaaij2018SimpleIntroduction,sharma2017MarkovChain}.
This builds up the posterior distribution by using a Markov chain, where each
step in the chain only depends on the previous step.  It starts by calculating
the posterior value for a particular point in parameter space. Then it will
randomly jump to another parameter space point, where a new value for the
posterior can be calculated.  If the new posterior value is higher than the
previous step then the jump is `accepted'. This just means that the parameter
values of this point are stored.  If the posterior value is lower than the
previous step then the jump is accepted with some probability~\chris{state what
this probability is. It's the ratio of probabilities at new and old locations}.
This means that the accepted positions are located around areas of high
posterior values, the \gls{MCMC} algorithm does not waste time calculating the
posterior in uninteresting areas of parameter space.  The accepted samples then
build the posterior distribution.~\chris{It's not clear what a "sample"
actually is and then how you wold use it to "build" a posterior distribution. A
bit more explanation would help the reader. Also this is a brief and reasonably
accurate MCMC description. You could opt to describe this algorithmically
rather than in text, just a thought.} 

In certain situations it can be useful to calculate the evidence~\chris{I think
Evidence throughout should be capitalised since it is a thing. You could also differentiate
it from the word evidence by referring to it as the Baysian Evidence. Just a
suggestion} in Eq.\ref{intro:bayes:evidence}.  For example, if there are two
different models which could represent the data, the evidence can be used to
determine which of the two models is more likely.  This is known as a Bayes
factor where two models $I_1$ and $I_2$ are compared and is defined as
%
\begin{equation}
B = \frac{p({\bm d} \mid I_1)}{p({\bm d} \mid I_2)}.
\end{equation}
%
~\chris{remember to puncuate your equations - full stops and commas after them.
I've corrected a few but not all.} This then requires the calculation of the
evidence.  To estimate the evidence efficiently a method known as Nested
sampling can be used, this is explained in detail in
\citep{skilling2006NestedSampling,speagle2019DynestyDynamic}~\chris{since you
don't use nested sampling in the thesis you don't need to go into detail in
explaining it but I might recommned a few more sentences than what you
currently have}. By calculating the Bayes factor, which is similar to a
likelihood ratio~\chris{true but is that relevant here since you haven't
introduced likelihood ratios?}, one can find the posterior odds~\chris{what
does "odds" mean?} of a particular
model~\chris{not a particular model but a particular pair of models} by using,
%
\begin{equation}
\frac{p(I_1 \mid \mathbf{d})}{p(I_2 \mid \mathbf{d})} = \frac{p(I_1)}{p(I_2)} \frac{p(\mathbf{d} \mid I_1)}{p(\mathbf{d} \mid I_2)}.
\end{equation}
%
The can be written as \textit{posterior odds = prior odds $\times$ Bayes
factor}~\chris{again what are odds, prior or posterior?}. This is then a
comparison of how likely different models are given some
observation~\chris{Expand this and say what the prior odds are and how the odds
ratio is a different thing from the Bayes Factor. They answer different
questions}. 

The methods described above then provide a way to estimate parameters of a
model given some data.  Also this provides a way to compare different models
given some observation.  In~\chris{the?} following sections the methods
described above are used to estimate various parameters.~\chris{that's not true
is it? You don't use MCMC or nested sampling in this chapter do you? Please
clarify or correct the statement.}


%%%%%%%%%%%55
%%%%%%%%%%%%%
\section{\label{searchcw:search} Continuous wave searches}
%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%

Searches for \glspl{CW} can be split into three general categories: Targeted
searches, Directed searches and All-sky searches, where the different
categories are based on the amount~\chris{number?} of source parameters which
are known~\chris{being "known" is not an easily definable thing. Do you clarify
this later?} prior to running the search.

%%%%%%
%%%%%%
\subsection{\label{searchcw:search:targeted}Targeted}
%%%%%%
%%%%%%

Targeted searches search~\chris{guns don't kill people, rappers do (Goldie
lookin chain). Searches don't search. Only scientists can search.} for specific
pulsars which have parameters known from electromagnetic observations, i.e.
X-ray, radio or $\gamma$-ray.  These can give the sky position parameters
$\alpha$ and $\delta$ and the source frequency parameters $f$ and its
derivatives, where there could also be extra parameters if the pulsar is in a
binary system~\chris{I think that you should define what "known" actually
means. There is always an error on parameters, eben from EM measurements.}.
Targeted searches can then use these parameters as input such that they search
over the remaining~\chris{unknown?} parameters $h_0, \iota, \phi_0, \psi$.  The
main targeted searches are the Bayesian time-domain search
\citep{dupuis2005BayesianEstimation}, the matched filter
$\mathcal{F}$-statistic \citep{schutz1998DataAnalysis} and the 5-Vector
approach \citep{astone2010MethodDetection}.

The Bayesian time-domain search is based on~\chris{not really based on this but
it is a starting point taking advantage of the narrow-band nature of the
signal} reducing the dataset to a manageable size such that Bayesian parameter
estimation can be applied with a reasonable computational cost.  This search
uses sky position and frequency parameters of the source to perform a slowly
evolving heterodyne which removes the phase evolution of the source
\citep{dupuis2005BayesianEstimation}~\chris{multiple "of the source"}  This allows
the signal to be low pass filtered and heavily downsampled without losing any
of the signal information.  This reduced dataset can then be used in a Bayesian
approach to search over the parameters $h_0, \iota, \phi_0, \psi$, see
\citep{dupuis2005BayesianEstimation} for further information.  The matched
filter $\mathcal{F}$-statistic uses the same heterodyned data as the Bayesian
method, howeverm, calculates a maximum likelihood $\mathcal{F}$-statistic
instead of a Bayesian posterior
\citep{schutz1998DataAnalysis,prix2007SearchContinuous,
aasi2014GRAVITATIONALWAVES}~\chris{things may have changed since I worked on
the F-stat code but are they definitely now using the heterodyned data as
input. It used to be SFTs.}.

The 5-Vector search is based in the frequency domain, which makes
useris{grammar - the frequency domain doesn't make use of anything - it's just
a domain.} of the five frequency harmonics caused by the sidereal amplitude
modulation caused byris{multiple "caused by"} the detectors antenna response as
the earth rotates \citep{astone2010MethodDetection,aasi2014GRAVITATIONALWAVES}.
A summary of the application of the searches for initial and advanced
\gls{LIGO} can be found in
\citep{aasi2014GRAVITATIONALWAVES,abbott2019SearchesGravitationala}.

Due to the long observation times needed to accumulate the required \gls{SNR}
for detection, most searches use data from an entire \gls{LIGO} observing run
which can last for $\mathcal{O}(1)$ years~\chris{should it be plural?}.  Given
that the sampling rate for the \gls{GW} channel is $16$ kHz (often downsampled
to $\sim 4$ kHz), the amount of data in a year can be up to $\mathcal{O}(2)$
terabytes, therefore these types of search can be computationally costly.
Whilst the fully coherent matched filter searches have methods to reduce the
computational time for known sources, in all-sky and directed searches, this
type of search is not feasible.  This is because all-sky and directed searches
have a wider parameter space, therefore, enough templates~\chris{the concept of
templates and matched-filtering have not been introduced.} need to be made to
sufficiently cover the large parameter space.  This task quickly becomes
impossible~\chris{never say impossible! It is infeasible or impractical. All
conditional on current computational resources.} for coherent matched filtering
for an entire observing run due to the amount of time needed. This problem led
to the development of semi-coherent searches which will be introduced in the
next section. 

%%%%%%
%%%%%%
\subsection{\label{searchcw:search:directed}Directed}
%%%%%%
%%%%%%

Directed searches generally know~\chris{searches know nothing - they are not
sentient.} some of the source parameters such as the sky position
$(\alpha,\delta)$ but not the rotation frequency~\chris{aren't directed
searcxhes exactlt that and only that? They're called directed becuase we know
the sky and little else right?}. This includes searches for neutron stars in
binary systems such as Sco-X1
\citep{abbott2017UpperLimits,meadors2016TuningScorpius}~\chris{there are a few
other LVC directed searches that have been performed including the galactic centre
search and a few known supernovae remnants - find the references.}.  These searches use
the similar techniques as all-sky searches, which will be described in
Sec.~\ref{searchcw:search:allsky}, they differ in that they can limit the
parameter space based on the known parameters.

%%%%%%
%%%%%%
\subsection{\label{searchcw:search:allsky}All-sky searches}
%%%%%%
%%%%%%

All-sky searches have no prior knowledge of the pulsars parameters, therefore,
~\chris{they are used to?} search over all the pulsar~\chris{in an all sky
search they wouldn't be pulsars since we don't see a pulse. They would still be
NSs though - or something unknown.} parameters $h_0, \iota, \psi, \phi_0, f,
\dot{f}, \alpha, \delta$. To use the techniques described in
Sec.~\ref{searchcw:search:targeted} for an all sky search, to sufficiently
cover the entire parameter space, the search would not be feasible~\chris{it
would not be feasible to run. The search cannot be "feasible".} to run due
to its large computational cost.  Instead a type of search known as a
semi-coherent search was developed.  These offered a solution to searching over
the large parameters space and data size.  The general idea of a semi-coherent
search is to break the dataset into smaller segments of length $T_{coh}$, which
each can be analysed coherently~\chris{there is also the sideband search which
splits data into different bands and not time segmenst - that is also
semi-coherent and publsihed by your supervisors in 2006ish}. The techniques described in
Sec.~\ref{searchcw:search:targeted} or another method such as a Fourier
transform can be used to analyse these small segments~\chris{this is very vague
and misleading sentence. The idea is that you still do a coherent search on the
small segment but the number of templates will be far smaller due to the shoter
time length. The new part is then how to combine all of the statistics from
each min-search.}.  The results from each
segment can the be combined incoherently using various methods which will be
summarised below.  This method can greatly reduce the computational cost for
the analysis depending on the coherence length, but will reduce the sensitivity
compared to the targeted~\chris{coherent} methods. 

There are many different types of semi-coherent search which use various
methods to incoherently combine the coherently analysed results.  I will
summarise some of these searches below,~\chris{grammar - not the place for a
comma} some of these searches were summarised and compared in
\citep{walsh2016ComparisonMethods}.  Many of these searches use a set of 1800s
long Fourier transforms as the input data, known as \glspl{SFT}.  This is a
default for many all-sky \gls{CW} searches, where it assumes that the signal
remains within one frequency bin during that 1800s~\chris{this last sentence is
loaded with questions. What is a bin? and I would ask if all SFT based searches
assume this approximation. I'm not sure that's true.}.

\begin{description}
	
        \item[Stack-slide] Stack~\chris{what is stack?} uses a set of Fourier
transforms of the data known as \glspl{SFT}, specifically it uses the power
spectrum of these~\chris{what is a power spectrum?}. Each of the separate
Fourier transforms (segments)~\chris{why rae we back to FFTs after saying we
use the power spectrum?} is shifted up or down~\chris{in frequency?} relative
to the others to account for the Doppler modulation of the source. The
power~\chris{what is power?} from each can then be stacked~\chris{what is
stacking? isn't it just adding?}. More explanation of this can be found in
\citep{brady2000SearchingPeriodic, cutler2005ImprovedStackslide}  
	
        \item[Hough] The Hough transform is based on the stack-slide algorithm.
The main difference is that the detection statistic for each segment is
assigned a weight of 0 or 1 depending if it crossed a detection threshold. The
Hough transform can then create a `Hough map' which gives a view of the data in
parameter space~\chris{very vague - be more specific.}. This approach is
explained in greater detail in
\citep{krishnan2004HoughTransform,antonucci2008DetectionPeriodic}.  This method
has been applied in two main ways known as Sky Hough
\citep{krishnan2004HoughTransform} and Frequency Hough
\citep{antonucci2008DetectionPeriodic,astone2014MethodAllsky}~\chris{any more
on that - what's the basic difference between these?}.
	
        \item[Einstein@Home] Einstein at home~\chris{It's always referred to as
Einstein@home - plus in these description lists the bold heading gives the
algorithm name and then you immediately repeat it. it looks weird.} uses the
$\mathcal{F}$-statistic mentioned above in various stages~\chris{was it
mentioned in variuos stages or used in various stages?}. It has a hierarchical
structure where it starts with a coarse parameter space with shorter coherence
times~\chris{that's the point of all semi-coherent searches}. This search then
provides a list of candidates from this run in coarse parameter
space~\chris{does it not semi-coherently combine results after the coarse
searches?}. The parameter space is then more finely sampled around the
parameters of the candidates~\chris{what are candidates?} and this process is
repeated.  The search can also increase the coherence length when searching
around given candidates to improve the sensitivity of the search. This
algorithm has many additions~\chris{what does "additions" mean? Does it do lots
of adding?} which are explained in more detail in
\citep{singh2016ResultsAllsky,papa2016HierarchicalFollowup,walsh2016ComparisonMethods}.
This~\chris{what is this?} provides the most sensitive all-sky \gls{CW} search,
however, uses a large amount of computing power. This is achieved by using a
distributed computing project, more details can be found at
\citep{EinsteinHome}. 
	
        \item[Time domain $\mathcal{F}$-statistic] The time domain
$\mathcal{F}$-statistic splits the data into narrowband segments of length
$\sim$ 2 days \citep{walsh2016ComparisonMethods}. Then a coherent search using
the $\mathcal{F}$-statistic is applied to each of these segments. Values of
this statistic above a threshold are stored. Coincidences are then found in
each segment, where candidates are selected best~\chris{based?} on a given
threshold. This is explained in greater detail in
\citep{aasi2014ImplementationTextdollar,walsh2016ComparisonMethods}.
	
        \item[Powerflux] Powerflux uses a standard set of 1800s \glspl{SFT}.
For each point in parameter space, the power~\chris{what is power?} in this set
of \glspl{SFT} along the frequency track~\chris{what frequency track?} is
recorded. This power is then weighted depending on the antenna pattern and
noise of the detector. In longer stretches of $\sim$ 1 month, the weighted
power is summed. Any point in parameter space which produces high power in each
of these stretches is identified as a potential signal. This search can then be
repeated around each candidate with a finer resolution in parameter space. This
is explained in more detail and tested in
\citep{abadie2012AllskySearch,walsh2016ComparisonMethods,ligoscientificcollaborationandvirgocollaboration2016ComprehensiveAllsky}
	
        \item[Viterbi] The Viterbi algorithm \citep{viterbi1967ErrorBounds} has
been used in \citep{sun2018HiddenMarkov,
suvorova2017HiddenMarkov,abbott2017SearchGravitational,
abbott2018SearchGravitational, sun2018ApplicationHidden} to search for a
\glspl{CW} with~\chris{unkown} randomly wandering spin frequency. This
algorithm was applied to specific sources, where the $\mathcal{F}$-statistic is
used on short duration segments which are then incoherently combined using the
Viterbi algorithm.
	
\end{description}

Each of these searches uses a large computational cost~\chris{grammar - you
don't really use a computational cost}. In \citep{walsh2016ComparisonMethods} a
\gls{MDC} was conducted to compare the sensitivity of some of the
searches,~\chris{remove comma and replace with "and"?} where an expected
runtime for an O1 search was presented. Results in O1~\chris{do we know what O1
is?} for some of these searches can now~\chris{why "now"? was it not there
earlier?} be found in
\citep{ligoscientificcollaborationandvirgocollaboration2017AllskySearch}.  The
results of this are shown in Tab.\ref{searchcw:search:semi:cost}~\chris{a bit
messy - short sharp sentences}.
%
\begin{table}
	\centering
	%
        \caption[Computational cost of \gls{CW} searches.]{From
\citep{walsh2016ComparisonMethods}, shows the computational cost for the first
4 months of advanced \gls{LIGO} for each search. One \gls{MSU}, where one
standard unit is one core-hour on a standard core.~\chris{what do these quotes
mean?} ` Expected computational
costs of searches using the first four months of advanced \gls{LIGO} data with
each search pipeline. These estimates are for a different data observing time
from that of the \gls{MDC}, and do not cover the same parameter space as each
other or the \gls{MDC}. The Einstein@Home searches uses the computing resources
of the Einstein@Home project and is designed to run for 6 - 10 months in the
Einstein@Home grid.'  \label{searchcw:search:semi:cost}}
	
	%
        \bgroup \def\arraystretch{1.5} \centering \begin{tabular}{|c c|} \hline
Pipeline & Expected runtime of O1 search \\ \hline Powerflux & 6.8 MSU \\

		Time domain $\mathcal{F}$-statistic & 1.6 MSU\\

		Frequency Hough & 0.9 MSU \\

		Sky Hough & 0.9 MSU\\
		\hline
		Einstein@Home & 100-170 MSU\\
		\hline

	\end{tabular}
	\egroup
\end{table}
%
Even the fastest of these searches takes close to 1 million core-hours to
search through four months of data.  This is one of the larger current issued
in \gls{CW} searches as this is a slow process and running computing clusters
can be costly.~\chris{typo or grammar issue here - I don't know what you're
trying to say.}


%%%%%%%%%5
%%%%%%%%%%%
\section{\label{searchcw:motivation}Motivation}
%%%%%%%%%%
%%%%%%%%%%

The searches described in Sec.~\ref{searchcw:search} are computationally
expensive, where the fastest takes $\sim$1 million core-hours to search through
4 months of O1 data.  Many of these searches use well-modelled signals to
compare to the data. This leads to the parameter space having to be finely
sampled such that it is sufficiently covered.  The motivation for much of the
work then follows from these points~\chris{which work? your work?}. The aim was
to develop searches which used minimal computational resources and could work
outside of the \gls{CW} model in Sec.~\ref{searchcw:model}~\chris{what does it
mena to work outside a model? Be more specific.}.  This thesis then
outlines~\chris{strange tense} algorithms which can reduce this computational
time of searches for \gls{CW} whilst retaining as much sensitivity to \gls{CW}
signals as possible~\chris{taken literally, this isn't true. You are knowingly
sacrificing sensitivity for speed.}.  There are two main sections which follow,
Sec.~\ref{soap} will outline an essentially un-modelled \gls{CW} search method
which uses the Viterbi algorithm.  Sec~\ref{machine}~\chris{don't start
sentences with abbreviations} will then outline a method which used~\chris{you
are all over the place with your tenses - past, present and future.} machine
learning, specifically \glspl{CNN} as both its own \gls{CW} search and an
extension to the search described in Sec.~\ref{soap}.  ~\chris{The?} Following
chapters then explain applications of these searches.~\chris{if this is where
you want to motivate the main chapters then you should expand on this a bit
more. One thing that you haven't mentioned is any benefit beyond simply using
less computational resources. That might be all we have but why is it good to
reduce the computational burden? To repeat myself, if you have only one section
in the thesis called "motivation" then it should be longer than a single
paragraph.}


